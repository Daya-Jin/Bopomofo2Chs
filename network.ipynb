{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1532803, 50) (1532803, 50)\n",
      "(2135, 50) (2135, 50)\n"
     ]
    }
   ],
   "source": [
    "from dataset import load_table, Dataset\n",
    "ch2id, spell2id = load_table()\n",
    "\n",
    "# 该配置下大概需要4G显存\n",
    "batch_size = 32\n",
    "len_thresh = (10, 50)  # 长度阈值\n",
    "\n",
    "train_data = Dataset('data/data_clean.txt', batch_size,\n",
    "                     len_thresh, shuffle=True)\n",
    "test_data = Dataset('eval/eval_clean.txt', batch_size,\n",
    "                    len_thresh, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "voc_size = len(ch2id)\n",
    "t_size = len_thresh[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, t_size])\n",
    "Y = tf.placeholder(tf.int32, [None, t_size])\n",
    "is_training = tf.placeholder(tf.bool)    # 训练标识位"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "emb_size = 300\n",
    "with tf.variable_scope('spell_emb'):\n",
    "    lookup_table = tf.get_variable(dtype=tf.float32, shape=[voc_size, emb_size],\n",
    "                                   initializer=tf.truncated_normal_initializer(mean=0,\n",
    "                                                                               stddev=0.01),\n",
    "                                   name='emb_lookup')\n",
    "    lookup_table = tf.concat((tf.zeros([1, emb_size]),\n",
    "                              lookup_table[1:, :]), axis=0)    # Empty对应的idx为0，将其emb全设为0\n",
    "    spell_emb = tf.nn.embedding_lookup(lookup_table, X)    # (None,t_size,emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-79174104cc4a>:7: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-79174104cc4a>:9: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# 论文中的Pre-net\n",
    "unit_fc = [emb_size, emb_size//2]\n",
    "drop_rate = 0.5\n",
    "\n",
    "with tf.variable_scope('Pre-net'):\n",
    "    prenet = tf.layers.dense(spell_emb, units=unit_fc[0],\n",
    "                             activation=tf.nn.relu)    # (None,t_size,unit_fc[0])\n",
    "    prenet = tf.layers.dropout(prenet, rate=drop_rate,\n",
    "                               training=is_training)    # (None,t_size,unit_fc[0])\n",
    "    prenet = tf.layers.dense(prenet, units=unit_fc[1],\n",
    "                             activation=tf.nn.relu)    # (None,t_size,unit_fc[1])\n",
    "    prenet = tf.layers.dropout(prenet, rate=drop_rate,\n",
    "                               training=is_training)    # (None,t_size,unit_fc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1D bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(inputs, training=True, act_f=None):\n",
    "    '''\n",
    "    自定义batch_norm，使用fusedBN\n",
    "    '''\n",
    "    # 把维度扩展成4维，然后使用更快的fusedBN\n",
    "    shape_I = inputs.get_shape()\n",
    "    rank_I = shape_I.ndims\n",
    "\n",
    "    if rank_I in [2, 3, 4]:\n",
    "        if rank_I == 2:\n",
    "            inputs = tf.expand_dims(inputs, axis=1)    # (X, X, 1)\n",
    "            inputs = tf.expand_dims(inputs, axis=2)    # (X, X, 1, 1)\n",
    "        elif rank_I == 3:\n",
    "            inputs = tf.expand_dims(inputs, axis=1)  # (X, X, X, 1)\n",
    "\n",
    "    inputs = tf.layers.batch_normalization(inputs, training=training,\n",
    "                                           fused=True)\n",
    "\n",
    "    # 恢复成原来的维度\n",
    "    if rank_I == 2:\n",
    "        inputs = tf.squeeze(inputs, axis=[1, 2])    # (X, X)\n",
    "    elif rank_I == 3:\n",
    "        inputs = tf.squeeze(inputs, axis=1)    # (X, X, X)\n",
    "\n",
    "    if act_f:\n",
    "        inputs = act_f(inputs)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-8769fa04e8d9>:8: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-8d85c93d3dac>:17: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-8769fa04e8d9>:22: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling1d instead.\n"
     ]
    }
   ],
   "source": [
    "K = 16\n",
    "with tf.variable_scope('Conv1D_bank'):\n",
    "    n_filters = emb_size//2\n",
    "\n",
    "    # 使用[1,K]个大小的卷积核提取信息，并拼接在一起，同TextCNN\n",
    "    # k=1\n",
    "    conv1d_bank = tf.layers.conv1d(prenet, filters=n_filters, kernel_size=1,\n",
    "                                   padding='same', use_bias=False)    # (None,t_size,n_filters)\n",
    "\n",
    "    # k=2,3,...,K\n",
    "    for k in range(2, K+1):\n",
    "        conv = tf.layers.conv1d(prenet, filters=n_filters, kernel_size=k,\n",
    "                                padding='same', use_bias=False)\n",
    "        conv1d_bank = tf.concat((conv1d_bank, conv),\n",
    "                                axis=-1)    # (None,t_size,k*n_filters)\n",
    "\n",
    "    conv1d_bank = batch_norm(conv1d_bank, training=is_training,\n",
    "                             act_f=tf.nn.relu)    # (None,t_size,K*n_filters)\n",
    "\n",
    "# 在t维度上做maxpool，同TextCNN\n",
    "max_pooling = tf.layers.max_pooling1d(conv1d_bank, pool_size=2, strides=1,\n",
    "                                      padding='same')    # (None,t_size,K*n_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1D projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1d_projections'):\n",
    "    conv1d_pro = tf.layers.conv1d(max_pooling, filters=n_filters, kernel_size=5,\n",
    "                                  padding='same', use_bias=False)    # (None,t_size,n_filters)\n",
    "    conv1d_pro = batch_norm(conv1d_pro, training=is_training, act_f=tf.nn.relu)\n",
    "\n",
    "    conv1d_pro = tf.layers.conv1d(conv1d_pro, filters=n_filters, kernel_size=5,\n",
    "                                  padding='same', use_bias=False)    # (None,t_size,n_filters)\n",
    "    conv1d_pro = batch_norm(conv1d_pro, training=is_training, act_f=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = prenet+conv1d_pro    # 残差连接，(None,t_size,n_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highway layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highway_block(inputs, units, scope=None):\n",
    "    '''\n",
    "    高速网络块\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        H = tf.layers.dense(inputs, units, activation=tf.nn.relu, name='H')\n",
    "        T = tf.layers.dense(inputs, units, activation=tf.nn.sigmoid,\n",
    "                            bias_initializer=tf.constant_initializer(-1.0), name='T')\n",
    "\n",
    "    return H*T+inputs*(1-T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_highway_block = 4\n",
    "for i in range(n_highway_block):\n",
    "    encoding = highway_block(encoding, units=emb_size//2,\n",
    "                             scope='highway_{}'.format(i))    # (None, t_size, emb_size//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-563cd47a6879>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-13-563cd47a6879>:5: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('BiRNN'):\n",
    "    gru_fw = tf.nn.rnn_cell.GRUCell(emb_size//2)\n",
    "    gru_bw = tf.nn.rnn_cell.GRUCell(emb_size//2)\n",
    "    rnn_out, _ = tf.nn.bidirectional_dynamic_rnn(gru_fw, gru_bw, encoding,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "    # (None,None,emb_size//2*2)，双向RNN*2\n",
    "    encoding = tf.concat(rnn_out, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-61f2c2d52864>:2: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n",
      "WARNING:tensorflow:From <ipython-input-14-61f2c2d52864>:2: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-61f2c2d52864>:6: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "logits = tf.layers.dense(encoding, len(ch2id), use_bias=False, name='logit')\n",
    "preds = tf.to_int32(tf.arg_max(logits, dimension=-1))\n",
    "\n",
    "with tf.name_scope('Eval'):\n",
    "    non_empty_mask = tf.to_float(tf.not_equal(\n",
    "        Y, tf.zeros_like(Y)))    # 0代表Empty，不参与计算\n",
    "    all_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y,\n",
    "                                                              logits=logits)    # 无差别loss\n",
    "    loss = tf.reduce_sum(all_loss*non_empty_mask) / \\\n",
    "        tf.reduce_sum(non_empty_mask)    # 非空loss\n",
    "    acc = tf.reduce_sum(tf.to_float(tf.equal(preds, Y)) * non_empty_mask) / \\\n",
    "        tf.reduce_sum(non_empty_mask)\n",
    "\n",
    "# train_op\n",
    "with tf.name_scope('train_op'):\n",
    "    lr = 1e-3\n",
    "    glob_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = tf.train.AdamOptimizer(lr) \\\n",
    "            .minimize(loss, global_step=glob_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True    # 按需使用显存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch_loss: 2.215376377105713, batch_acc: 0.6859421730041504\n",
      "epoch: 1, batch_loss: 1.5334725379943848, batch_acc: 0.7321063280105591\n",
      "epoch: 1, batch_loss: 1.2114081382751465, batch_acc: 0.7555321455001831\n",
      "epoch: 1, batch_loss: 0.9934782981872559, batch_acc: 0.7843137383460999\n",
      "epoch: 1, batch_loss: 0.7513704895973206, batch_acc: 0.8320935368537903\n",
      "epoch: 1, batch_loss: 0.6397449374198914, batch_acc: 0.8294416069984436\n",
      "epoch: 1, test_acc: 0.8576877117156982\n",
      "epoch: 1, batch_loss: 0.5762260556221008, batch_acc: 0.8477580547332764\n",
      "epoch: 1, batch_loss: 0.4585951268672943, batch_acc: 0.8772093057632446\n",
      "epoch: 1, batch_loss: 0.4294915795326233, batch_acc: 0.8851774334907532\n",
      "epoch: 1, batch_loss: 0.4553367793560028, batch_acc: 0.8808837532997131\n",
      "epoch: 1, batch_loss: 0.4109019637107849, batch_acc: 0.8803879022598267\n",
      "epoch: 1, batch_loss: 0.4234901964664459, batch_acc: 0.8888888955116272\n",
      "epoch: 1, test_acc: 0.9053552746772766\n",
      "epoch: 1, batch_loss: 0.3857410252094269, batch_acc: 0.8908709287643433\n",
      "epoch: 1, batch_loss: 0.3474505841732025, batch_acc: 0.9016715884208679\n",
      "epoch: 1, batch_loss: 0.29379117488861084, batch_acc: 0.9187562465667725\n",
      "epoch: 1, batch_loss: 0.34297695755958557, batch_acc: 0.8936170339584351\n",
      "epoch: 1, batch_loss: 0.3786940276622772, batch_acc: 0.8971962332725525\n",
      "epoch: 1, batch_loss: 0.3974323868751526, batch_acc: 0.8941532373428345\n",
      "epoch: 1, test_acc: 0.9222856760025024\n",
      "epoch: 1, batch_loss: 0.3341744542121887, batch_acc: 0.9033970236778259\n",
      "epoch: 1, batch_loss: 0.3131808340549469, batch_acc: 0.9083743691444397\n",
      "epoch: 1, batch_loss: 0.29916030168533325, batch_acc: 0.903292179107666\n",
      "epoch: 1, batch_loss: 0.30490124225616455, batch_acc: 0.913943350315094\n",
      "epoch: 1, batch_loss: 0.27157309651374817, batch_acc: 0.920127809047699\n",
      "epoch: 1, batch_loss: 0.2808866798877716, batch_acc: 0.9180819392204285\n",
      "epoch: 1, test_acc: 0.9290164709091187\n",
      "epoch: 1, batch_loss: 0.2734672725200653, batch_acc: 0.9179179072380066\n",
      "epoch: 1, batch_loss: 0.30043384432792664, batch_acc: 0.9136822819709778\n",
      "epoch: 1, batch_loss: 0.26385965943336487, batch_acc: 0.9373219609260559\n",
      "epoch: 1, batch_loss: 0.28267306089401245, batch_acc: 0.9279749393463135\n",
      "epoch: 1, batch_loss: 0.1938284933567047, batch_acc: 0.9445958137512207\n",
      "epoch: 1, batch_loss: 0.28548863530158997, batch_acc: 0.9189789295196533\n",
      "epoch: 1, test_acc: 0.9322437644004822\n",
      "epoch: 1, batch_loss: 0.31410640478134155, batch_acc: 0.9120458960533142\n",
      "epoch: 1, batch_loss: 0.2594762444496155, batch_acc: 0.9220907092094421\n",
      "epoch: 1, batch_loss: 0.31237807869911194, batch_acc: 0.9158607125282288\n",
      "epoch: 1, batch_loss: 0.23921705782413483, batch_acc: 0.9310710430145264\n",
      "epoch: 1, batch_loss: 0.3172532021999359, batch_acc: 0.9089979529380798\n",
      "epoch: 1, batch_loss: 0.2506817877292633, batch_acc: 0.9193697571754456\n",
      "epoch: 1, test_acc: 0.9394749999046326\n",
      "epoch: 1, batch_loss: 0.2457483410835266, batch_acc: 0.9295065402984619\n",
      "epoch: 1, batch_loss: 0.28895246982574463, batch_acc: 0.930962324142456\n",
      "epoch: 1, batch_loss: 0.21745063364505768, batch_acc: 0.9360780119895935\n",
      "epoch: 1, batch_loss: 0.2045358121395111, batch_acc: 0.9356846213340759\n",
      "epoch: 1, batch_loss: 0.2938520610332489, batch_acc: 0.9251559376716614\n",
      "epoch: 1, batch_loss: 0.20299512147903442, batch_acc: 0.9452736377716064\n",
      "epoch: 1, test_acc: 0.9435997605323792\n",
      "epoch: 1, batch_loss: 0.20337148010730743, batch_acc: 0.9480269551277161\n",
      "epoch: 1, batch_loss: 0.25578591227531433, batch_acc: 0.9249011874198914\n",
      "epoch: 1, batch_loss: 0.24442321062088013, batch_acc: 0.9350649118423462\n",
      "epoch: 1, batch_loss: 0.25809913873672485, batch_acc: 0.9277978539466858\n",
      "epoch: 1, batch_loss: 0.2282029390335083, batch_acc: 0.9311926364898682\n",
      "epoch: 1, batch_loss: 0.22147773206233978, batch_acc: 0.9364919066429138\n",
      "epoch: 1, test_acc: 0.9418048858642578\n",
      "epoch: 1, batch_loss: 0.22019612789154053, batch_acc: 0.9333986043930054\n",
      "epoch: 1, batch_loss: 0.20332208275794983, batch_acc: 0.9382591247558594\n",
      "epoch: 1, batch_loss: 0.22417040169239044, batch_acc: 0.9286453127861023\n",
      "epoch: 1, batch_loss: 0.2232854813337326, batch_acc: 0.9290322661399841\n",
      "epoch: 1, batch_loss: 0.25537410378456116, batch_acc: 0.9312499761581421\n",
      "epoch: 1, batch_loss: 0.20392721891403198, batch_acc: 0.9433611631393433\n",
      "epoch: 1, test_acc: 0.9453428387641907\n",
      "epoch: 1, batch_loss: 0.18821276724338531, batch_acc: 0.9413549304008484\n",
      "epoch: 1, batch_loss: 0.2193959355354309, batch_acc: 0.9408983588218689\n",
      "epoch: 1, batch_loss: 0.17742174863815308, batch_acc: 0.947119951248169\n",
      "epoch: 1, batch_loss: 0.22369708120822906, batch_acc: 0.9425641298294067\n",
      "epoch: 1, batch_loss: 0.25438645482063293, batch_acc: 0.9294003844261169\n",
      "epoch: 1, batch_loss: 0.2722782790660858, batch_acc: 0.9270427823066711\n",
      "epoch: 1, test_acc: 0.9454981684684753\n",
      "epoch: 1, batch_loss: 0.2459443211555481, batch_acc: 0.9332706928253174\n",
      "epoch: 1, batch_loss: 0.17009522020816803, batch_acc: 0.9515366554260254\n",
      "epoch: 1, batch_loss: 0.3036818206310272, batch_acc: 0.9242262244224548\n",
      "epoch: 1, batch_loss: 0.2093074768781662, batch_acc: 0.9449275135993958\n",
      "epoch: 1, batch_loss: 0.20929887890815735, batch_acc: 0.9363538026809692\n",
      "epoch: 1, batch_loss: 0.16617195308208466, batch_acc: 0.9458897113800049\n",
      "epoch: 1, test_acc: 0.9489498138427734\n",
      "epoch: 1, batch_loss: 0.16354016959667206, batch_acc: 0.9521126747131348\n",
      "epoch: 1, batch_loss: 0.3024680018424988, batch_acc: 0.9110122323036194\n",
      "epoch: 1, batch_loss: 0.1380387544631958, batch_acc: 0.9616182446479797\n",
      "epoch: 1, batch_loss: 0.21953564882278442, batch_acc: 0.941717803478241\n",
      "epoch: 1, batch_loss: 0.17690739035606384, batch_acc: 0.9477317333221436\n",
      "epoch: 1, batch_loss: 0.19197581708431244, batch_acc: 0.9414732456207275\n",
      "epoch: 1, test_acc: 0.949087917804718\n",
      "epoch: 1, batch_loss: 0.2340218424797058, batch_acc: 0.9294478297233582\n",
      "epoch: 1, batch_loss: 0.23762719333171844, batch_acc: 0.9334006309509277\n",
      "epoch: 1, batch_loss: 0.24389883875846863, batch_acc: 0.9327309131622314\n",
      "epoch: 1, batch_loss: 0.1706143468618393, batch_acc: 0.9444444179534912\n",
      "epoch: 1, batch_loss: 0.19696177542209625, batch_acc: 0.9507278800010681\n",
      "epoch: 1, batch_loss: 0.17237098515033722, batch_acc: 0.9459202885627747\n",
      "epoch: 1, test_acc: 0.9512969851493835\n",
      "epoch: 1, batch_loss: 0.18158811330795288, batch_acc: 0.9436008930206299\n",
      "epoch: 1, batch_loss: 0.1893819272518158, batch_acc: 0.9479392766952515\n",
      "epoch: 1, batch_loss: 0.23601746559143066, batch_acc: 0.9377828240394592\n",
      "epoch: 1, batch_loss: 0.18602772057056427, batch_acc: 0.9416058659553528\n",
      "epoch: 1, batch_loss: 0.2206353098154068, batch_acc: 0.9326065182685852\n",
      "epoch: 1, batch_loss: 0.16614876687526703, batch_acc: 0.9520676732063293\n",
      "epoch: 1, test_acc: 0.9514005184173584\n",
      "epoch: 1, batch_loss: 0.21051223576068878, batch_acc: 0.9544236063957214\n",
      "epoch: 1, batch_loss: 0.1670449823141098, batch_acc: 0.9461457133293152\n",
      "epoch: 1, batch_loss: 0.1671990603208542, batch_acc: 0.9598130583763123\n",
      "epoch: 1, batch_loss: 0.21687421202659607, batch_acc: 0.9401444792747498\n",
      "epoch: 1, batch_loss: 0.2491409033536911, batch_acc: 0.9326424598693848\n",
      "epoch: 1, batch_loss: 0.22758686542510986, batch_acc: 0.9305936098098755\n",
      "epoch: 1, test_acc: 0.9529192447662354\n",
      "epoch: 1, batch_loss: 0.15482144057750702, batch_acc: 0.9561671614646912\n",
      "epoch: 1, batch_loss: 0.14640328288078308, batch_acc: 0.9524886608123779\n",
      "epoch: 1, batch_loss: 0.18327032029628754, batch_acc: 0.949367105960846\n",
      "epoch: 1, batch_loss: 0.20587097108364105, batch_acc: 0.9468749761581421\n",
      "epoch: 1, batch_loss: 0.1934581696987152, batch_acc: 0.9431230425834656\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 1    # 太慢了，只跑一次\n",
    "\n",
    "    batch_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in train_data.next_batch():\n",
    "            batch_cnt += 1\n",
    "            loss_val, acc_val, _ = sess.run([loss, acc, train_op],\n",
    "                                            feed_dict={X: batch_data, Y: batch_labels,\n",
    "                                                       is_training: True})\n",
    "\n",
    "            if batch_cnt % 500 == 0:\n",
    "                print('epoch: {}, batch_loss: {}, batch_acc: {}'\n",
    "                      .format(epoch+1, loss_val, acc_val))\n",
    "\n",
    "            if batch_cnt % 3000 == 0:\n",
    "                test_acc_val = sess.run(acc, feed_dict={X: test_data.data, Y: test_data.target,\n",
    "                                                        is_training: False})\n",
    "                print('epoch: {}, test_acc: {}'.format(epoch+1, test_acc_val))\n",
    "\n",
    "    Y_pred = sess.run(preds, feed_dict={X: test_data.data, Y: test_data.target,\n",
    "                                        is_training: False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "id2ch = pickle.load(open('data/id2ch.pkl', 'rb'))\n",
    "\n",
    "with open('eval/eval_res.csv', 'w', encoding='utf-8') as fd:\n",
    "    fd.write('True,Pred,CER\\n')\n",
    "    total_cer = 0\n",
    "\n",
    "    for y_test, y_pred in zip(test_data.target, Y_pred):\n",
    "        s_len = np.count_nonzero(y_test)\n",
    "        y_test_ch = ''.join([id2ch[idx]\n",
    "                             for idx in y_test])[:s_len].replace('_', '')\n",
    "        y_pred_ch = ''.join([id2ch[idx]\n",
    "                             for idx in y_pred])[:s_len].replace('_', '')\n",
    "        cer = distance.levenshtein(y_test_ch, y_pred_ch)/s_len\n",
    "\n",
    "        fd.write('{},{},{:.2f}\\n'.format(y_test_ch, y_pred_ch, cer))\n",
    "\n",
    "        total_cer += cer\n",
    "\n",
    "    fd.write('Total CER: {:.2f}\\n'.format(total_cer/test_data.target.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
